{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Word2Vec on HackerNews\n",
    "## Assumptions\n",
    "This notebook assumes the following:\n",
    "- This script assumes that the collection is stored by years\n",
    "- Currently, we hardcode the years to 2016, 2017, 2018 which fits the scope of the project\n",
    "\n",
    "## Requirements\n",
    "In order to run the notebook, you have to do the following:\n",
    "- source activate hackernews\n",
    "- import the data into MongoDB <br>\n",
    "```mongoimport --db HackerNews --collections hn_{{ $year }} {{ $year }}.fmt```\n",
    "\n",
    "## Sources\n",
    "https://rare-technologies.com/word2vec-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data\n",
    "## Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.HackerNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hn_2016 = db.hn_2016\n",
    "# hn_2017 = db.hn_2017\n",
    "hn_2018 = db.hn_2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = hn_2016.find_one()\n",
    "cursor['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = hn_2016.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering via months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_dt = date(1970, 1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {'jan': 1,\n",
    "         'feb': 2,\n",
    "         'mar': 3,\n",
    "         'apr': 4,\n",
    "         'may': 5,\n",
    "         'jun': 6,\n",
    "         'jul': 7,\n",
    "         'aug': 8,\n",
    "         'sep': 9,\n",
    "         'oct': 10,\n",
    "         'nov': 11,\n",
    "         'dec': 12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_interested = 'jan'\n",
    "month = months[month_interested]\n",
    "start_time = int((date(2016, month, 1) - epoch_dt).total_seconds()) \n",
    "end_time = int((date(2016, month+1, 1) - epoch_dt).total_seconds())\n",
    "\n",
    "start_time, end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = hn_2016.find_one()\n",
    "cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entries = []\n",
    "entries_text = []\n",
    "for entry in cursor:\n",
    "    if int(entry['time']) > start_time and int(entry['time']) < end_time:\n",
    "#         entries.append(entry)\n",
    "        entries_text.append(entry['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(entries_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the text\n",
    "1) takes in unclean text <br>\n",
    "2) clean(text) <br>\n",
    "    - unescape, remove tags, unneccessary spaces, decontracted\n",
    "3) sent_tokenize <br>\n",
    "4) clean_2 <br>\n",
    "5) word_tokenize <br>\n",
    "6) clean_3 <br>\n",
    "    - stopwords removal, pos_tag, only accept noun, verb, adj, adv, and lemmatize noun and verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import html\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    clean_text = html.unescape(text)\n",
    "    clean_text = re.sub(r'\\\\n', ' ', clean_text)\n",
    "    clean_text = re.sub(r'<a.*</a>',' ', clean_text)\n",
    "    clean_text = re.sub(r'<p.*</p>', ' ', clean_text)\n",
    "    clean_text = re.sub(r'<.?>', ' ', clean_text)\n",
    "    clean_text = re.sub(r'</.?>', ' ', clean_text)\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    def decontracted(phrase):\n",
    "        # specific\n",
    "        phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "        # general\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "        phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        return phrase\n",
    "    clean_text = decontracted(clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def clean_2(text):\n",
    "    # remove punctuations\n",
    "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    clean_text = regex.sub('', text)\n",
    "    return clean_text\n",
    "\n",
    "def clean_3(tokens):\n",
    "    clean_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "    pos = nltk.pos_tag(clean_tokens, tagset='universal')\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    new_tokens = []\n",
    "    accepted_pos = ['NOUN', 'VERB', 'ADJ','ADV']\n",
    "    to_lemmatize = ['NOUN', 'VERB']\n",
    "    change_dict = {'NOUN':'n',\n",
    "                 'VERB':'v',\n",
    "                 'ADJ':'a',\n",
    "                 'ADV':'r'}\n",
    "    for i in pos:\n",
    "        if i[-1] in accepted_pos:\n",
    "            temp = i[0]\n",
    "            if i[-1] in to_lemmatize:\n",
    "                temp = wnl.lemmatize(temp, pos = change_dict[i[-1]])\n",
    "            temp.lower()\n",
    "            new_tokens.append(temp.lower())\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries_text[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entries_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c17858ee6f93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entries_text' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = ''\n",
    "for i in entries_text:\n",
    "    if len(clean(i)) < 5:\n",
    "        continue\n",
    "    if clean(i)[-1] != '.':\n",
    "        corpus += clean(i)[:-1] + '.'\n",
    "    else:\n",
    "        corpus += clean(i)\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = corpus\n",
    "clean_test = clean(test)\n",
    "clean_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(clean_test)\n",
    "for i in sentences:\n",
    "    print(i)\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = [nltk.word_tokenize(clean_2(sentence)) for sentence in sentences]\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = [clean_3(tokens) for tokens in sentences]\n",
    "for sent in test:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c866c62f76f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mclean_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "clean_text = clean(corpus)\n",
    "sentences = nltk.sent_tokenize(clean_text)\n",
    "sentences = [nltk.word_tokenize(clean_2(sentence)) for sentence in sentences]\n",
    "sentences = [clean_3(tokens) for tokens in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpus: 4\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2aedc54f14b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcpu_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of cpus: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mw2v_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcpu_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# w2v_model.save('model')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "cpu_count = multiprocessing.cpu_count()\n",
    "print(\"Number of cpus: {}\".format(cpu_count))\n",
    "w2v_model = Word2Vec(sentences, size = 100, window = 20, min_count =5, workers = cpu_count, seed = 123)\n",
    "# w2v_model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85953"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = w2v_model.wv.vocab\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renjie/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/renjie/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('physician', 0.7035205364227295),\n",
       " ('patient', 0.694062352180481),\n",
       " ('doctors', 0.6193013191223145),\n",
       " ('hospital', 0.6171454787254333),\n",
       " ('patients', 0.616409182548523)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.most_similar(positive=['doctor', 'male'], negative = ['female'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renjie/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/renjie/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24809471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.similarity('doctor', 'male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/renjie/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('orca', 0.6142606735229492),\n",
       " ('surfer', 0.5859272480010986),\n",
       " ('specimen', 0.5731604099273682),\n",
       " ('tiger', 0.5649954080581665),\n",
       " ('dwarves', 0.5575430393218994),\n",
       " ('eggplant', 0.5490908622741699),\n",
       " ('den', 0.5466058254241943),\n",
       " ('roo', 0.5462160110473633),\n",
       " ('fur', 0.5430614948272705),\n",
       " ('moose', 0.5396895408630371)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar = w2v_model.wv.most_similar('')\n",
    "similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "vectors = []\n",
    "for word in w2v_model.wv.vocab:\n",
    "    vectors.append(w2v_model[word])\n",
    "    labels.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(perplexity=20, n_components= 3, init = 'pca', n_iter = 250, random_state=123)\n",
    "new_values = tsne_model.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(len(new_values)):\n",
    "    value = new_values[i]\n",
    "    plt.scatter(value[0], value[1], value[2], alpha=0.5, color = 'steelblue')\n",
    "    plt.annotate(labels[i],\n",
    "                xy = (value[0], value[1], value[2]),\n",
    "                xytext = (5,2),\n",
    "                textcoords = 'offset points',\n",
    "                ha = 'right',\n",
    "                va = 'bottom',\n",
    "                alpha = 0.7)\n",
    "\n",
    "ax.set_axis_off()\n",
    "# ax.set_xlabel('X Label')\n",
    "# ax.set_ylabel('Y Label')\n",
    "# ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.wv.accuracy('questions-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65719"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = hn_2016.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jan 1514764800 1517443200\n",
      "Number of entries:  234992\n",
      "Number of sentences:  515331\n",
      "model saved\n",
      "model corpus size: 87430, time elapsed: 2175.947058200836\n",
      "feb 1517443200 1519862400\n",
      "Number of entries:  209738\n",
      "Number of sentences:  483225\n",
      "model saved\n",
      "model corpus size: 88568, time elapsed: 4204.82554769516\n",
      "mar 1519862400 1522540800\n",
      "Number of entries:  237342\n",
      "Number of sentences:  528557\n",
      "model saved\n",
      "model corpus size: 89937, time elapsed: 6471.897032499313\n",
      "apr 1522540800 1525132800\n",
      "Number of entries:  237609\n",
      "Number of sentences:  534533\n",
      "model saved\n",
      "model corpus size: 91431, time elapsed: 9552.911533594131\n",
      "may 1525132800 1527811200\n",
      "Number of entries:  237648\n",
      "Number of sentences:  528362\n",
      "model saved\n",
      "model corpus size: 92790, time elapsed: 12288.604754686356\n",
      "jun 1527811200 1530403200\n",
      "Number of entries:  231815\n",
      "Number of sentences:  529024\n",
      "model saved\n",
      "model corpus size: 94101, time elapsed: 14585.267132520676\n",
      "jul 1530403200 1533081600\n",
      "Number of entries:  224880\n",
      "Number of sentences:  520389\n",
      "model saved\n",
      "model corpus size: 95358, time elapsed: 16775.824457883835\n",
      "aug 1533081600 1535760000\n",
      "Number of entries:  230758\n",
      "Number of sentences:  522439\n",
      "model saved\n",
      "model corpus size: 96551, time elapsed: 19457.683418273926\n",
      "sep 1535760000 1538352000\n",
      "Number of entries:  61991\n",
      "Number of sentences:  144917\n",
      "model saved\n",
      "model corpus size: 96769, time elapsed: 20359.662579774857\n",
      "oct 1538352000 1541030400\n",
      "Number of entries:  0\n",
      "Number of sentences:  0\n",
      "model saved\n",
      "model corpus size: 96769, time elapsed: 20378.102284908295\n",
      "nov 1541030400 1543622400\n",
      "Number of entries:  0\n",
      "Number of sentences:  0\n",
      "model saved\n",
      "model corpus size: 96769, time elapsed: 20395.758898973465\n",
      "dec 1543622400 1546300800\n",
      "Number of entries:  0\n",
      "Number of sentences:  0\n",
      "model saved\n",
      "model corpus size: 96769, time elapsed: 20413.76545882225\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "month_shortform = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "\n",
    "for month_interested in month_shortform:\n",
    "    cursor = hn_2018.find()\n",
    "    if month_interested == 'dec':\n",
    "        start_time = int((date(2018, 12, 1) - epoch_dt).total_seconds()) \n",
    "        end_time = int((date(2018+1, 1, 1) - epoch_dt).total_seconds())\n",
    "    else:\n",
    "        month = months[month_interested]\n",
    "        start_time = int((date(2018, month, 1) - epoch_dt).total_seconds()) \n",
    "        end_time = int((date(2018, month+1, 1) - epoch_dt).total_seconds())\n",
    "    \n",
    "    print(month_interested, start_time, end_time)\n",
    "    entries_text = []\n",
    "    for entry in cursor:\n",
    "        if int(entry['time']) > start_time and int(entry['time']) < end_time:\n",
    "            entries_text.append(entry['text'])\n",
    "    print('Number of entries: ',len(entries_text))\n",
    "    corpus = ''\n",
    "    for i in entries_text:\n",
    "        if len(clean(i)) < 5:\n",
    "            continue\n",
    "        if clean(i)[-1] != '.':\n",
    "            corpus += clean(i)[:-1] + '.'\n",
    "        else:\n",
    "            corpus += clean(i)\n",
    "    clean_text = clean(corpus)\n",
    "    sentences = nltk.sent_tokenize(clean_text)\n",
    "    sentences = [nltk.word_tokenize(clean_2(sentence)) for sentence in sentences]\n",
    "    sentences = [clean_3(tokens) for tokens in sentences]\n",
    "    print('Number of sentences: ',len(sentences))\n",
    "    w2v_model.build_vocab(sentences, update=True)\n",
    "    w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 10)\n",
    "    w2v_model.save('model')\n",
    "    print('model saved')\n",
    "    print('model corpus size: {}, time elapsed: {}'.format(len(w2v_model.wv.vocab), time.time() - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences, update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.train(sentences, total_examples = w2v_model.corpus_count, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.corpus_count, w2v_model.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model['raimi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
